{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning InternLM2 with LoRA on A100 GPU\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook provides a detailed guide to fine-tuning the InternLM2 model for sequence classification tasks using Low-Rank Adaptation (LoRA) on an A100 GPU. The process covers everything from setting up the environment and installing dependencies to preprocessing data, configuring the model, and performing efficient training. The final steps include saving and exporting the fine-tuned model for deployment.\n",
    "\n",
    "### Settings and Configuration\n",
    "\n",
    "- **Model:** InternLM2 with Low-Rank Adaptation (LoRA)\n",
    "- **Hardware:** GPU (A100, 40GB)\n",
    "- **Precision:** Mixed Precision (FP16)\n",
    "- **Batch Size:** 8 (per device)\n",
    "- **Learning Rate:** 1e-6\n",
    "- **Optimization Strategy:** AdamW with 8-bit precision\n",
    "- **Number of Epochs:** 1\n",
    "- **Training Data:** Custom dataset loaded from CSV\n",
    "- **Tokenization:** Managed using AutoTokenizer with a max sequence length of 1024 tokens\n",
    "\n",
    "### Key Features\n",
    "\n",
    "- **Low-Rank Adaptation (LoRA):** Efficient adaptation of large models to specific tasks with reduced computational overhead.\n",
    "- **GPU Acceleration:** Utilization of A100 GPUs for high-performance training.\n",
    "- **Custom Data Processing:** Tailored preprocessing and tokenization to handle complex input data structures.\n",
    "- **Efficient Model Saving:** Techniques for saving the trained model, including merging LoRA layers into the base model for seamless deployment.\n",
    "\n",
    "### Created By\n",
    "\n",
    "This notebook was created by Sathya Narayanan Balamurugan to demonstrate the efficient fine-tuning of large language models using LoRA on high-performance GPUs, with a focus on practical implementation and deployment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "snapshot_download(repo_id=\"SathyaVrv/Lmsys-InternLM\",local_dir='~/downloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in ./.local/lib/python3.10/site-packages (23.3.1)\n",
      "Collecting pip\n",
      "  Downloading pip-24.2-py3-none-any.whl.metadata (3.6 kB)\n",
      "Downloading pip-24.2-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 23.3.1\n",
      "    Uninstalling pip-23.3.1:\n",
      "      Successfully uninstalled pip-23.3.1\n",
      "Successfully installed pip-24.2\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers>=4.42.3\n",
      "  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m159.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting bitsandbytes\n",
      "  Downloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting peft\n",
      "  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers>=4.42.3) (3.6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.42.3) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.42.3) (1.25.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers>=4.42.3) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers>=4.42.3) (5.4.1)\n",
      "Collecting regex!=2019.12.17 (from transformers>=4.42.3)\n",
      "  Downloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.42.3) (2.31.0)\n",
      "Collecting safetensors>=0.4.1 (from transformers>=4.42.3)\n",
      "  Downloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers>=4.42.3)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.42.3) (4.66.1)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (from bitsandbytes) (2.0.1)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.42.3) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers>=4.42.3) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers>=4.42.3) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers>=4.42.3) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers>=4.42.3) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers>=4.42.3) (2020.6.20)\n",
      "Downloading transformers-4.43.3-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading bitsandbytes-0.43.3-py3-none-manylinux_2_24_x86_64.whl (137.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.5/137.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.7.24-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (776 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m776.5/776.5 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m83.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m103.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: flatbuffers 1.12.1-git20200711.33e2d80-dfsg1-0.6 has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of flatbuffers or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: safetensors, regex, bitsandbytes, tokenizers, accelerate, transformers, peft\n",
      "Successfully installed accelerate-0.33.0 bitsandbytes-0.43.3 peft-0.12.0 regex-2024.7.24 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.43.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.20.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.10/site-packages (from datasets) (1.25.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets) (1.3.5)\n",
      "Collecting requests>=2.32.2 (from datasets)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tqdm>=4.66.3 (from datasets)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in ./.local/lib/python3.10/site-packages (from datasets) (0.24.5)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.3.4-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.local/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
      "Collecting async-timeout<5.0,>=4.0 (from aiohttp->datasets)\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.10/site-packages (from huggingface-hub>=0.21.2->datasets) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Downloading datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "Downloading aiohttp-3.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m161.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "Downloading aiohappyeyeballs-2.3.4-py3-none-any.whl (12 kB)\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Downloading frozenlist-1.4.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (239 kB)\n",
      "Downloading multidict-6.0.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (124 kB)\n",
      "Downloading yarl-1.9.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (301 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: xxhash, tqdm, requests, pyarrow-hotfix, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.1\n",
      "    Uninstalling fsspec-2024.6.1:\n",
      "      Successfully uninstalled fsspec-2024.6.1\n",
      "Successfully installed aiohappyeyeballs-2.3.4 aiohttp-3.10.1 aiosignal-1.3.1 async-timeout-4.0.3 datasets-2.20.0 dill-0.3.8 frozenlist-1.4.1 fsspec-2024.5.0 multidict-6.0.5 multiprocess-0.70.16 pyarrow-17.0.0 pyarrow-hotfix-0.6 requests-2.32.3 tqdm-4.66.5 xxhash-3.4.1 yarl-1.9.4\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting einops\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: einops\n",
      "Successfully installed einops-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Downloading sentencepiece-0.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m35.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (1.3.5)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in ./.local/lib/python3.10/site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas) (2022.1)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2024.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m110.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tzdata, pandas\n",
      "Successfully installed pandas-2.2.2 tzdata-2024.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (3.0.3)\n",
      "Collecting jinja2\n",
      "  Downloading jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2) (2.0.1)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "\u001b[33mWARNING: Error parsing dependencies of flatbuffers: Invalid version: '1.12.1-git20200711.33e2d80-dfsg1-0.6'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: jinja2\n",
      "Successfully installed jinja2-3.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Specify the directory containing the modules\n",
    "directory = 'modeling-internlm'\n",
    "\n",
    "# Add the directory to sys.path so Python can find the modules\n",
    "sys.path.append(directory)\n",
    "\n",
    "# Load the configuration module first to avoid ImportError\n",
    "config_spec = importlib.util.spec_from_file_location(\"configuration_internlm2\", os.path.join(directory, \"configuration_internlm2.py\"))\n",
    "configuration_internlm2 = importlib.util.module_from_spec(config_spec)\n",
    "config_spec.loader.exec_module(configuration_internlm2)\n",
    "\n",
    "# Now load the modeling_internlm2 module\n",
    "spec = importlib.util.spec_from_file_location(\"modeling_internlm2\", os.path.join(directory, \"modeling_internlm2.py\"))\n",
    "modeling_internlm2 = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(modeling_internlm2)\n",
    "\n",
    "# Add the modules to sys.modules\n",
    "sys.modules[\"configuration_internlm2\"] = configuration_internlm2\n",
    "sys.modules[\"modeling_internlm2\"] = modeling_internlm2\n",
    "\n",
    "# Now you can import InternLM2Model\n",
    "from modeling_internlm2 import *\n",
    "from configuration_internlm2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "/home/ubuntu/.local/lib/python3.10/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.2' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType,PeftModel\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    6095\n",
      "1    5901\n",
      "2    5481\n",
      "Name: count, dtype: int64\n",
      "17477\n",
      "label\n",
      "0    6095\n",
      "1    5901\n",
      "2    5481\n",
      "Name: count, dtype: int64\n",
      "label\n",
      "0    6095\n",
      "1    5901\n",
      "2    5481\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "TRAIN_CSV = \"data/train.csv\"\n",
    "#EVAL_CSV = \"data/eval_set.csv\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "#eval_df = pd.read_csv(EVAL_CSV)\n",
    "\n",
    "# Select the last 30,000 rows and reverse the order\n",
    "#train = train.iloc[-11000:][::-1].reset_index(drop=True)\n",
    "# Display the first few rows to verify\n",
    "train = train[40000:]\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]\n",
    "print(train['label'].value_counts())\n",
    "\n",
    "print(len(train))\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(train['label'].value_counts())\n",
    "\n",
    "# Calculate the number of samples per label\n",
    "# Determine the unique labels\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(train['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# Step 1: Calculate the text length in the original DataFrame\n",
    "train['text_length'] = train['prompt'].apply(len) + train['response_a'].apply(len) + train['response_b'].apply(len)\n",
    "\n",
    "# Sort the DataFrame based on text length in descending order (longest texts first)\n",
    "train_sorted = train.sort_values(by='text_length', ascending=False)\n",
    "\n",
    "# Select the top 5% longest texts directly\n",
    "subsample_size_20 = int(0.05 * len(df))\n",
    "train_sorted = train_sorted.head(subsample_size_20)\n",
    "\n",
    "merged_df = pd.concat([train_sorted, eval_df]).drop_duplicates().reset_index(drop=True)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('tokenizer/tokenizer_config.json',\n",
       " 'tokenizer/special_tokens_map.json',\n",
       " 'tokenizer/./tokenizer.model',\n",
       " 'tokenizer/added_tokens.json',\n",
       " 'tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm2-7b-reward\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "# save tokenizer to load offline during inference\n",
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chat(tokenizer, conversation):\n",
    "    # Apply the chat template and encode the conversation\n",
    "    conversation_str = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)\n",
    "    # Encode without adding special tokens automatically\n",
    "    input_ids = tokenizer.encode(conversation_str, return_tensors=\"pt\", add_special_tokens=False,padding=\"max_length\", truncation=True, max_length=MAX_LENGTH)\n",
    "\n",
    "    # Check if the last token is not equal to 92527\n",
    "    if input_ids[0, -1] != 92527:\n",
    "        if input_ids.shape[1] >= MAX_LENGTH:\n",
    "            # Remove the last word if max length is reached\n",
    "            input_ids = input_ids[:, :-1]\n",
    "        \n",
    "        # Add the token 92527 at the end\n",
    "        input_ids = torch.cat([input_ids, torch.tensor([[92527]], dtype=torch.long)], dim=1)\n",
    "\n",
    "    # Ensure that the input_ids are exactly MAX_LENGTH tokens long\n",
    "    if input_ids.shape[1] > MAX_LENGTH:\n",
    "        input_ids = input_ids[:, :MAX_LENGTH]\n",
    "    \n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "def get_tokens(tokenizer, prompt, response_a, response_b):\n",
    "    # Prepare the conversation with assistant_a and assistant_b roles\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant_a\", \"content\": response_a},\n",
    "        {\"role\": \"assistant_b\", \"content\": response_b}\n",
    "    ]\n",
    "    \n",
    "    # Tokenize the entire conversation\n",
    "    tokens = preprocess_chat(tokenizer, chat)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = example['prompt']\n",
    "    response_a = example['response_a']\n",
    "    response_b = example['response_b']\n",
    "    \n",
    "    # Get tokens for both assistant_a and assistant_b content\n",
    "    tokens = get_tokens(tokenizer, prompt, response_a, response_b)\n",
    "    \n",
    "    # Extract input IDs and attention mask\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "    # If exceeding max length, truncate\n",
    "    if input_ids.size(1) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:, :MAX_LENGTH - 1]\n",
    "        attention_mask = attention_mask[:, :MAX_LENGTH - 1]\n",
    "\n",
    "    # Assign label: 0 for Response A, 1 for Response B, 2 for Tie\n",
    "    label = int(example['label'])  # Convert label to an integer based on your label encoding\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids.squeeze(0).tolist(),  # Flatten to list\n",
    "        \"attention_mask\": attention_mask.squeeze(0).tolist(),  # Flatten to list\n",
    "        \"labels\": label\n",
    "    }\n",
    "\n",
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03d31ff4dae14efca124df30a4d77ce5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/17477 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    print(labels)\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)\n",
    "    y_pred=preds.argmax(-1)\n",
    "    print(y_pred)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "    return {\"acc\": acc, \"log_loss\": loss}\n",
    "\n",
    "n_splits = 100\n",
    "fold_idx = 0\n",
    "ds = load_data(train, tokenizer)\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = train.iloc[eval_idx]\n",
    "\n",
    "# Step 2: Save the filtered DataFrame to a CSV file\n",
    "eval_df.to_csv('data/eval_set-3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92527"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[0]['input_ids'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model configuration\n",
    "config = AutoConfig.from_pretrained(\"quantized-model\", num_labels=3,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "# Load the model with ignore_mismatched_sizes=True\n",
    "model = InternLM2ForSequenceClassification.from_pretrained(\n",
    "    'quantized-model',\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto',\n",
    "    torch_dtype=torch.float16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 379,338,752\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Assuming `model` is your initialized model\n",
    "total_params = count_parameters(model)\n",
    "\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InternLM2ForSequenceClassification(\n",
       "  (model): InternLM2Model(\n",
       "    (tok_embeddings): Embedding(92544, 4096, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x InternLM2DecoderLayer(\n",
       "        (attention): InternLM2Attention(\n",
       "          (wqkv): Linear4bit(in_features=4096, out_features=6144, bias=False)\n",
       "          (wo): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): InternLM2RotaryEmbedding()\n",
       "        )\n",
       "        (feed_forward): InternLM2MLP(\n",
       "          (w1): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (w3): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (w2): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (attention_norm): InternLM2RMSNorm()\n",
       "        (ffn_norm): InternLM2RMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): InternLM2RMSNorm()\n",
       "  )\n",
       "  (v_head): Linear(in_features=4096, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['wqkv', 'wo','w1','w2','w3'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForSequenceClassification(\n",
      "  (base_model): LoraModel(\n",
      "    (model): InternLM2ForSequenceClassification(\n",
      "      (model): InternLM2Model(\n",
      "        (tok_embeddings): Embedding(92544, 4096, padding_idx=2)\n",
      "        (layers): ModuleList(\n",
      "          (0-31): 32 x InternLM2DecoderLayer(\n",
      "            (attention): InternLM2Attention(\n",
      "              (wqkv): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=6144, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=6144, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (wo): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (rotary_emb): InternLM2RotaryEmbedding()\n",
      "            )\n",
      "            (feed_forward): InternLM2MLP(\n",
      "              (w1): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (w3): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=14336, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (w2): lora.Linear4bit(\n",
      "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=14336, out_features=16, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (attention_norm): InternLM2RMSNorm()\n",
      "            (ffn_norm): InternLM2RMSNorm()\n",
      "          )\n",
      "        )\n",
      "        (norm): InternLM2RMSNorm()\n",
      "      )\n",
      "      (v_head): Linear(in_features=4096, out_features=3, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "trainable params: 37,748,736 || all params: 7,396,409,344 || trainable%: 0.5104\n"
     ]
    }
   ],
   "source": [
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the state_dict from the pytorch_model.bin file\n",
    "state_dict = torch.load('output/H100-full_model_pytorch-2/pytorch_model.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys: []\n",
      "Unexpected keys: []\n"
     ]
    }
   ],
   "source": [
    "# Load the weights into the model, excluding the new target modules\n",
    "missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "# Print out any missing or unexpected keys for debugging\n",
    "print(f\"Missing keys: {missing_keys}\")\n",
    "print(f\"Unexpected keys: {unexpected_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 37,748,736\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Assuming `model` is your initialized model\n",
    "total_params = count_parameters(model)\n",
    "\n",
    "print(f\"Total trainable parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameter: base_model.model.model.layers.0.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.0.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.1.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.2.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.3.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.4.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.5.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.6.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.7.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.8.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.9.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.10.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.11.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.12.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.13.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.14.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.15.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.16.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.17.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.18.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.19.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.20.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.21.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.22.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.23.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.24.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.25.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.26.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.27.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.28.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.29.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.30.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.attention.wqkv.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.attention.wqkv.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.attention.wo.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.attention.wo.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.feed_forward.w1.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.feed_forward.w1.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.feed_forward.w3.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.feed_forward.w3.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.feed_forward.w2.lora_A.default.weight\n",
      "Trainable parameter: base_model.model.model.layers.31.feed_forward.w2.lora_B.default.weight\n",
      "Trainable parameter: base_model.model.v_head.weight\n"
     ]
    }
   ],
   "source": [
    "# Unfreeze LoRA and v_head parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name or \"v_head\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Optionally, you can print out which parameters are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable parameter: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 37,761,024 || all params: 7,396,409,344 || trainable%: 0.5105\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='output-h100-3',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=750,  # Save every 75 steps\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=1,  # Log every 1 steps\n",
    "    warmup_steps=150,  # Warmup steps\n",
    "    optim=\"adamw_8bit\",\n",
    "    learning_rate=1e-6,  # Learning rate # 5e-6 # 7.5e-6 # 1e-6\n",
    "    per_device_train_batch_size=8,  # Batch size of 8 sequences\n",
    "    per_device_eval_batch_size=32, \n",
    "    gradient_accumulation_steps=2,  # Effectively 2 gradient accumulation\n",
    "    max_grad_norm=0.4,  # Gradient clipping\n",
    "    num_train_epochs=1,  # Number of epochs\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Mixed precision training\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Starting to train ##\n"
     ]
    }
   ],
   "source": [
    "print(\"## Starting to train ##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1081' max='1081' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1081/1081 2:37:43, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Acc</th>\n",
       "      <th>Log Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.959600</td>\n",
       "      <td>0.944280</td>\n",
       "      <td>0.411429</td>\n",
       "      <td>1.132267</td>\n",
       "      <td>26.977600</td>\n",
       "      <td>6.487000</td>\n",
       "      <td>0.222000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 0 1 2 2 1 1 0 2 2 2 2 2 2 2 2 2 1 0 1 1 0 0 1 1 0 2 1 2 2 0 1 0 1\n",
      " 0 2 0 0 0 0 2 1 0 2 1 1 0 0 2 0 2 0 0 1 0 2 1 1 2 1 1 1 0 0 2 1 1 0 2 1 2\n",
      " 2 2 0 0 0 0 0 0 1 1 0 2 0 1 1 1 2 0 0 1 1 0 1 1 2 2 1 2 1 1 1 0 1 1 0 1 0\n",
      " 1 0 2 0 2 2 2 0 1 1 2 1 0 2 2 1 0 0 0 1 0 1 0 0 1 0 2 1 1 2 1 1 2 0 0 2 1\n",
      " 1 2 0 1 1 2 2 0 2 1 2 1 0 0 1 0 2 1 0 2 0 0 1 0 0 2 0]\n",
      "[2 1 2 0 0 0 0 0 0 1 1 2 1 0 2 2 2 1 0 0 1 2 2 1 0 2 2 0 2 1 2 0 2 0 1 1 1\n",
      " 0 2 2 0 0 2 1 1 0 2 2 1 0 0 1 0 2 1 1 2 2 0 0 1 2 1 2 2 2 1 2 2 0 1 2 1 2\n",
      " 2 2 0 0 1 0 1 2 0 1 1 2 1 0 2 1 1 0 1 2 1 2 2 2 0 1 1 0 0 0 1 1 0 2 0 1 2\n",
      " 1 2 1 0 1 0 0 2 2 1 2 0 1 2 0 0 0 2 1 0 0 0 0 0 0 2 1 1 1 0 2 2 1 0 2 1 0\n",
      " 0 2 0 0 1 2 0 1 2 1 0 1 0 2 1 2 1 2 1 1 2 0 1 0 2 1 0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1081, training_loss=0.8671687980915638, metrics={'train_runtime': 9472.3499, 'train_samples_per_second': 1.827, 'train_steps_per_second': 0.114, 'total_flos': 7.457100004117709e+17, 'train_loss': 0.8671687980915638, 'epoch': 0.9995376791493297})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'output/H100-full_model_pytorch-3'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save the full model, including the v_head layer\n",
    "torch.save(model.state_dict(), os.path.join(save_path, 'pytorch_model.bin'))\n",
    "\n",
    "# Save the configuration as well\n",
    "model.config.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:336: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = model.merge_and_unload()\n",
    "model.save_pretrained('H100-merged-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
