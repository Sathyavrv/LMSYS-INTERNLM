{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":9035948,"sourceType":"datasetVersion","datasetId":5446759},{"sourceId":190417743,"sourceType":"kernelVersion"},{"sourceId":81881,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102},{"sourceId":82484,"sourceType":"modelInstanceVersion","modelInstanceId":69297,"modelId":94427}],"dockerImageVersionId":30734,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine-Tuning LLaMA for Sequence Classification with LoRA and TPU Acceleration\n\n## Overview\n\nThis notebook provides a comprehensive guide to fine-tuning a LLaMA model for sequence classification tasks using Low-Rank Adaptation (LoRA) in a TPU-accelerated environment. The project showcases best practices for handling large-scale data, optimizing model performance, and leveraging TPUs for efficient computation.\n\n### Settings and Configuration\n\n- **Model:** LLaMA-based Sequence Classification Model\n- **Adaptation Technique:** Low-Rank Adaptation (LoRA)\n- **Hardware:** TPU (Tensor Processing Unit)\n- **Precision:** Mixed Precision (bfloat16)\n- **Batch Size:** 16\n- **Optimizer:** AdamW with Cosine Learning Rate Scheduling\n- **Epochs:** 6\n- **Max Sequence Length:** 1024 tokens\n- **Dropout Rate:** 0.05\n- **Learning Rate:** Maximum 5e-5 with warm-up steps\n- **Sharding:** Implemented using TPUâ€™s SPMD (Single Program Multiple Data) for distributed training\n\n### Created By\n\nThis notebook was created and fine-tuned by Sathya Narayanan Balamurugan, demonstrating a methodical approach to model training and optimization in a high-performance computing environment.\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Import libs ","metadata":{}},{"cell_type":"code","source":"# Install libs\n!pip install -qq peft==0.6.0\n!pip install -qq bitsandbytes==0.41.1\n!pip install -qq accelerate==0.24.1\n!pip install -qq transformers==4.43.2\n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q \n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:26:57.773143Z","iopub.execute_input":"2024-07-25T04:26:57.773479Z","iopub.status.idle":"2024-07-25T04:28:58.051579Z","shell.execute_reply.started":"2024-07-25T04:26:57.773450Z","shell.execute_reply":"2024-07-25T04:28:58.050389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nimport torch.nn.functional as F\n\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\nfrom spmd_util import partition_module\n\ntqdm.pandas()\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:28:58.053664Z","iopub.execute_input":"2024-07-25T04:28:58.053980Z","iopub.status.idle":"2024-07-25T04:29:12.398562Z","shell.execute_reply.started":"2024-07-25T04:28:58.053948Z","shell.execute_reply":"2024-07-25T04:29:12.397763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"class CFG:\n    NUM_EPOCHS = 6\n    BATCH_SIZE = 16\n    DROPOUT = 0.05 \n    MODEL_NAME = '/kaggle/input/llama-3.1/transformers/8b-instruct/1'\n    SEED = 2024 \n    MAX_LENGTH = 1024 \n    NUM_WARMUP_STEPS = 128\n    LR_MAX = 5e-5 \n    NUM_LABELS = 3 \n    LORA_RANK = 4\n    LORA_ALPHA = 8\n    LORA_MODULES = ['o_proj', 'v_proj']\n    \nDEVICE = xm.xla_device() # Initialize TPU Device","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:12.399541Z","iopub.execute_input":"2024-07-25T04:29:12.399974Z","iopub.status.idle":"2024-07-25T04:29:12.403833Z","shell.execute_reply.started":"2024-07-25T04:29:12.399946Z","shell.execute_reply":"2024-07-25T04:29:12.403249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n    # Set seed for all TPU cores\n    xm.set_rng_state(seed, device=xm.xla_device())  \n\nset_seeds(seed=CFG.SEED)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:12.404669Z","iopub.execute_input":"2024-07-25T04:29:12.404916Z","iopub.status.idle":"2024-07-25T04:29:12.418659Z","shell.execute_reply.started":"2024-07-25T04:29:12.404892Z","shell.execute_reply":"2024-07-25T04:29:12.417974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n\n# save tokenizer to load offline during inference\ntokenizer.save_pretrained('tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:12.420699Z","iopub.execute_input":"2024-07-25T04:29:12.420990Z","iopub.status.idle":"2024-07-25T04:29:13.053182Z","shell.execute_reply.started":"2024-07-25T04:29:12.420959Z","shell.execute_reply":"2024-07-25T04:29:13.052454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function giving token length\ndef get_token_lengths(texts):\n    # tokenize and receive input_ids for reach text\n    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n    # return length of inputs_ids for each text\n    return [len(t) for t in input_ids]","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:13.054067Z","iopub.execute_input":"2024-07-25T04:29:13.054313Z","iopub.status.idle":"2024-07-25T04:29:13.057979Z","shell.execute_reply.started":"2024-07-25T04:29:13.054289Z","shell.execute_reply":"2024-07-25T04:29:13.057301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare train\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\n# Drop 'Null' for training\nindexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\ntrain.drop(indexes, inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nprint(f\"Total {len(indexes)} Null response rows dropped\")\nprint('Total train samples: ', len(train))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:13.058857Z","iopub.execute_input":"2024-07-25T04:29:13.059117Z","iopub.status.idle":"2024-07-25T04:29:16.964413Z","shell.execute_reply.started":"2024-07-25T04:29:13.059091Z","shell.execute_reply":"2024-07-25T04:29:16.963698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:16.965361Z","iopub.execute_input":"2024-07-25T04:29:16.965612Z","iopub.status.idle":"2024-07-25T04:29:16.978671Z","shell.execute_reply.started":"2024-07-25T04:29:16.965587Z","shell.execute_reply":"2024-07-25T04:29:16.977956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\nprint(train['text'][4])","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:16.979632Z","iopub.execute_input":"2024-07-25T04:29:16.979905Z","iopub.status.idle":"2024-07-25T04:29:17.290482Z","shell.execute_reply.started":"2024-07-25T04:29:16.979861Z","shell.execute_reply":"2024-07-25T04:29:17.289734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train with only take 50% train dataset\ntrain = train[:int(len(train) * 0.5)]\n\ntrain.loc[:, 'token_count'] = get_token_lengths(train['text'])\n\n# prepare label for model\ntrain.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n\n# Display data\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:17.291355Z","iopub.execute_input":"2024-07-25T04:29:17.291594Z","iopub.status.idle":"2024-07-25T04:29:31.948640Z","shell.execute_reply.started":"2024-07-25T04:29:17.291570Z","shell.execute_reply":"2024-07-25T04:29:31.947772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:31.949616Z","iopub.execute_input":"2024-07-25T04:29:31.949936Z","iopub.status.idle":"2024-07-25T04:29:31.956083Z","shell.execute_reply.started":"2024-07-25T04:29:31.949905Z","shell.execute_reply":"2024-07-25T04:29:31.955405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# token Count\ndisplay(train['token_count'].describe().to_frame().astype(int))","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:31.956975Z","iopub.execute_input":"2024-07-25T04:29:31.957227Z","iopub.status.idle":"2024-07-25T04:29:31.975778Z","shell.execute_reply.started":"2024-07-25T04:29:31.957201Z","shell.execute_reply":"2024-07-25T04:29:31.975136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get length of tokens which covers 90% of data, we'll still take 1024 length!\nnp.percentile(train['token_count'], 90)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:31.976575Z","iopub.execute_input":"2024-07-25T04:29:31.976799Z","iopub.status.idle":"2024-07-25T04:29:31.981972Z","shell.execute_reply.started":"2024-07-25T04:29:31.976776Z","shell.execute_reply":"2024-07-25T04:29:31.981296Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize","metadata":{}},{"cell_type":"code","source":"# Tokenize Data\ntokens = tokenizer(\n    train['text'].tolist(), \n    padding='max_length', \n    max_length=CFG.MAX_LENGTH, \n    truncation=True, \n    return_tensors='np')\n\n# Input IDs are the token IDs\nINPUT_IDS = tokens['input_ids']\n# Attention Masks to Ignore Padding Tokens\nATTENTION_MASKS = tokens['attention_mask']\n# Label of Texts\nLABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n\nprint(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\nprint(f'LABELS shape: {LABELS.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:31.984751Z","iopub.execute_input":"2024-07-25T04:29:31.985157Z","iopub.status.idle":"2024-07-25T04:29:46.730386Z","shell.execute_reply.started":"2024-07-25T04:29:31.985129Z","shell.execute_reply":"2024-07-25T04:29:46.729663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_dataset(batch_size):\n    N_SAMPLES = LABELS.shape[0]\n    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n    while True:\n        # Shuffle Indices\n        np.random.shuffle(IDXS)\n        # Iterate Over All Indices Once\n        for idxs in IDXS.reshape(-1, batch_size):\n            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output\n            \n            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            \n            yield input_ids, attention_mask, labels\n\nTRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:46.731526Z","iopub.execute_input":"2024-07-25T04:29:46.731839Z","iopub.status.idle":"2024-07-25T04:29:46.737711Z","shell.execute_reply.started":"2024-07-25T04:29:46.731808Z","shell.execute_reply":"2024-07-25T04:29:46.737071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"# Load model for classification with 3 target label\nbase_model = LlamaForSequenceClassification.from_pretrained(\n    CFG.MODEL_NAME,\n    num_labels=CFG.NUM_LABELS,\n    torch_dtype=torch.bfloat16)\n\nbase_model.config.pretraining_tp = 1 \n\n# Assign Padding TOKEN\nbase_model.config.pad_token_id = tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:46.738670Z","iopub.execute_input":"2024-07-25T04:29:46.738929Z","iopub.status.idle":"2024-07-25T04:29:48.433143Z","shell.execute_reply.started":"2024-07-25T04:29:46.738904Z","shell.execute_reply":"2024-07-25T04:29:48.432415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Low-Rank Adaptation [LORA]","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices\n    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations\n    lora_dropout= CFG.DROPOUT, \n    bias='none',\n    inference_mode=False,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:48.434055Z","iopub.execute_input":"2024-07-25T04:29:48.434307Z","iopub.status.idle":"2024-07-25T04:29:48.437965Z","shell.execute_reply.started":"2024-07-25T04:29:48.434281Z","shell.execute_reply":"2024-07-25T04:29:48.437346Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create LoRa Model\nmodel = get_peft_model(base_model, lora_config)\n# Trainable Parameters\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:48.438809Z","iopub.execute_input":"2024-07-25T04:29:48.439089Z","iopub.status.idle":"2024-07-25T04:29:48.528786Z","shell.execute_reply.started":"2024-07-25T04:29:48.439062Z","shell.execute_reply":"2024-07-25T04:29:48.528144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of TPU Nodes\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n# distribute model\npartition_module(model, mesh)\n\nprint(f'num_devices: {num_devices}')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:29:48.529668Z","iopub.execute_input":"2024-07-25T04:29:48.530149Z","iopub.status.idle":"2024-07-25T04:30:29.249594Z","shell.execute_reply.started":"2024-07-25T04:29:48.530120Z","shell.execute_reply":"2024-07-25T04:30:29.248716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verfy The Trainable Layers\nMODEL_LAYERS_ROWS = []\nTRAINABLE_PARAMS = []\nN_TRAINABLE_PARAMS = 0\n\nfor name, param in model.named_parameters():\n    # Layer Parameter Count\n    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n    # Only Trainable Layers\n    if param.requires_grad:\n        # Add Layer Information\n        MODEL_LAYERS_ROWS.append({\n            'param': n_parameters,\n            'name': name,\n            'dtype': param.data.dtype,\n        })\n        # Append Trainable Parameter\n        TRAINABLE_PARAMS.append({ 'params': param })\n        # Add Number Of Trainable Parameters\"\n        N_TRAINABLE_PARAMS += n_parameters\n        \ndisplay(pd.DataFrame(MODEL_LAYERS_ROWS))\n\nprint(f\"\"\"\n===============================\nN_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\nN_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n===============================\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:30:29.250569Z","iopub.execute_input":"2024-07-25T04:30:29.250817Z","iopub.status.idle":"2024-07-25T04:30:29.271482Z","shell.execute_reply.started":"2024-07-25T04:30:29.250792Z","shell.execute_reply":"2024-07-25T04:30:29.270812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# LR & Optimizer\nN_SAMPLES = len(train)\nSTEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n\nOPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)\n\n# Cosine Learning Rate With Warmup\nlr_scheduler = transformers.get_cosine_schedule_with_warmup(\n    optimizer=OPTIMIZER,\n    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)\n\nprint(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:30:29.272256Z","iopub.execute_input":"2024-07-25T04:30:29.272474Z","iopub.status.idle":"2024-07-25T04:30:29.285831Z","shell.execute_reply.started":"2024-07-25T04:30:29.272451Z","shell.execute_reply":"2024-07-25T04:30:29.285059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the data type for the optimizer's state (e.g., momentum buffers)\nfor state in OPTIMIZER.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n            state[v] = v.to(dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:30:29.286737Z","iopub.execute_input":"2024-07-25T04:30:29.286977Z","iopub.status.idle":"2024-07-25T04:30:29.300644Z","shell.execute_reply.started":"2024-07-25T04:30:29.286953Z","shell.execute_reply":"2024-07-25T04:30:29.300020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids, attention_mask, labels = next(TRAIN_DATASET)\n\nprint(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\nprint(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\nprint(f'labels shape: {labels.shape}, dtype: {labels.dtype}')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:30:29.301479Z","iopub.execute_input":"2024-07-25T04:30:29.301695Z","iopub.status.idle":"2024-07-25T04:30:29.315666Z","shell.execute_reply.started":"2024-07-25T04:30:29.301673Z","shell.execute_reply":"2024-07-25T04:30:29.314914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Dummy Prediction\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    \nprint(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:30:29.316599Z","iopub.execute_input":"2024-07-25T04:30:29.316838Z","iopub.status.idle":"2024-07-25T04:30:56.575676Z","shell.execute_reply.started":"2024-07-25T04:30:29.316813Z","shell.execute_reply":"2024-07-25T04:30:56.574920Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put Model In Train Mode\nmodel.train()\n\n# Loss Function, Cross Entropy\nLOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:30:56.576633Z","iopub.execute_input":"2024-07-25T04:30:56.576948Z","iopub.status.idle":"2024-07-25T04:30:56.584776Z","shell.execute_reply.started":"2024-07-25T04:30:56.576917Z","shell.execute_reply":"2024-07-25T04:30:56.584014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time()\nwarnings.filterwarnings(\"error\")\nMETRICS = {\n    'loss': [],\n    'accuracy': {'y_true': [], 'y_pred': [] }}\n\nfor epoch in tqdm(range(CFG.NUM_EPOCHS)):\n    ste = time()\n    for step in range(STEPS_PER_EPOCH):\n        # Zero Out Gradients\n        OPTIMIZER.zero_grad()\n        \n        # Get Batch\n        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n        \n        # Forward Pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n       \n        # Logits Float32\n        logits = outputs.logits.to(dtype=torch.float32)\n        \n        # Backward Pass\n        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n        loss.backward()\n        \n        # optimizer step\n        OPTIMIZER.step()\n        xm.mark_step()\n        \n        # Update Learning Rate Scheduler\n        lr_scheduler.step()\n        \n        # Update Metrics And Progress Bar\n        METRICS['loss'].append(float(loss))\n        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()\n        \n        if (step + 1) % 200 == 0:  \n            metrics = 'Âµ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n            metrics += ', Âµ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n                                                               METRICS['accuracy']['y_pred']))\n            lr = OPTIMIZER.param_groups[0]['lr']\n            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n            print(f'\\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')\n    \n    print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )\n\n    # If stopped, and to continue training in future on tpu we save model and optimizer\n    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')\n    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    \n    \n    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')","metadata":{"execution":{"iopub.status.busy":"2024-07-25T04:30:56.585660Z","iopub.execute_input":"2024-07-25T04:30:56.585887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nplt.plot(METRICS['loss'])    \nplt.xlabel('Step per epoch')\nplt.ylabel('Loss')\nplt.title('Loss Plot step per epoch')    \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model\n","metadata":{}},{"cell_type":"code","source":"model = model.cpu()\ntorch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}