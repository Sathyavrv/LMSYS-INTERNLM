{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install einops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Specify the directory containing the modules\n",
    "directory = 'modeling-internlm'\n",
    "\n",
    "# Add the directory to sys.path so Python can find the modules\n",
    "sys.path.append(directory)\n",
    "\n",
    "# Load the configuration module first to avoid ImportError\n",
    "config_spec = importlib.util.spec_from_file_location(\"configuration_internlm2\", os.path.join(directory, \"configuration_internlm2.py\"))\n",
    "configuration_internlm2 = importlib.util.module_from_spec(config_spec)\n",
    "config_spec.loader.exec_module(configuration_internlm2)\n",
    "\n",
    "# Now load the modeling_internlm2 module\n",
    "spec = importlib.util.spec_from_file_location(\"modeling_internlm2\", os.path.join(directory, \"modeling_internlm2.py\"))\n",
    "modeling_internlm2 = importlib.util.module_from_spec(spec)\n",
    "spec.loader.exec_module(modeling_internlm2)\n",
    "\n",
    "# Add the modules to sys.modules\n",
    "sys.modules[\"configuration_internlm2\"] = configuration_internlm2\n",
    "sys.modules[\"modeling_internlm2\"] = modeling_internlm2\n",
    "\n",
    "# Now you can import InternLM2Model\n",
    "from modeling_internlm2 import *\n",
    "from configuration_internlm2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType,PeftModel\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.utils import resample\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample\n",
    "TRAIN_CSV = \"data/train.csv\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "\n",
    "# Select the last 30,000 rows and reverse the order\n",
    "#train = train.iloc[-11000:][::-1].reset_index(drop=True)\n",
    "# Display the first few rows to verify\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]\n",
    "print(train['label'].value_counts())\n",
    "\n",
    "print(len(train))\n",
    "\n",
    "train = train.head(500)\n",
    "\n",
    "# Display the first few rows to verify the changes\n",
    "print(train['label'].value_counts())\n",
    "\n",
    "# Calculate the number of samples per label\n",
    "# Determine the unique labels\n",
    "\n",
    "# Display the first few rows to verify\n",
    "print(train['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"internlm/internlm2-7b-reward\", trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "# save tokenizer to load offline during inference\n",
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chat(tokenizer, conversation):\n",
    "    # Apply the chat template and encode the conversation\n",
    "    conversation_str = tokenizer.apply_chat_template(conversation, tokenize=False, add_generation_prompt=False)\n",
    "    # Encode without adding special tokens automatically\n",
    "    input_ids = tokenizer.encode(conversation_str, return_tensors=\"pt\", add_special_tokens=False,padding=\"max_length\", truncation=True, max_length=MAX_LENGTH - 1)\n",
    "    \n",
    "    input_ids = torch.cat([input_ids, torch.tensor([[92527]], dtype=torch.long)], dim=1)\n",
    "    # Create attention mask\n",
    "    attention_mask = torch.ones_like(input_ids, dtype=torch.bool)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n",
    "\n",
    "def get_tokens(tokenizer, prompt, response_a, response_b):\n",
    "    # Prepare the conversation with assistant_a and assistant_b roles\n",
    "    chat = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant_a\", \"content\": response_a},\n",
    "        {\"role\": \"assistant_b\", \"content\": response_b}\n",
    "    ]\n",
    "    \n",
    "    # Tokenize the entire conversation\n",
    "    tokens = preprocess_chat(tokenizer, chat)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = example['prompt']\n",
    "    response_a = example['response_a']\n",
    "    response_b = example['response_b']\n",
    "    \n",
    "    # Get tokens for both assistant_a and assistant_b content\n",
    "    tokens = get_tokens(tokenizer, prompt, response_a, response_b)\n",
    "    \n",
    "    # Extract input IDs and attention mask\n",
    "    input_ids = tokens[\"input_ids\"]\n",
    "    attention_mask = tokens[\"attention_mask\"]\n",
    "\n",
    "    # If exceeding max length, truncate\n",
    "    if input_ids.size(1) > MAX_LENGTH:\n",
    "        input_ids = input_ids[:, :MAX_LENGTH - 1]\n",
    "        attention_mask = attention_mask[:, :MAX_LENGTH - 1]\n",
    "\n",
    "    # Assign label: 0 for Response A, 1 for Response B, 2 for Tie\n",
    "    label = int(example['label'])  # Convert label to an integer based on your label encoding\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids.squeeze(0).tolist(),  # Flatten to list\n",
    "        \"attention_mask\": attention_mask.squeeze(0).tolist(),  # Flatten to list\n",
    "        \"labels\": label\n",
    "    }\n",
    "\n",
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    print(labels)\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)\n",
    "    y_pred=preds.argmax(-1)\n",
    "    print(y_pred)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "    return {\"acc\": acc, \"log_loss\": loss}\n",
    "\n",
    "n_splits = 20\n",
    "fold_idx = 0\n",
    "ds = load_data(train, tokenizer)\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds[0]['input_ids'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model configuration\n",
    "config = AutoConfig.from_pretrained(\"/kaggle/input/internlm_quantified_7b/transformers/default/1\", num_labels=3,trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model with ignore_mismatched_sizes=True\n",
    "model = InternLM2ForSequenceClassification.from_pretrained(\n",
    "    '/kaggle/input/internlm_quantified_7b/transformers/default/1',\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Assuming `model` is your initialized model\n",
    "total_params = count_parameters(model)\n",
    "\n",
    "print(f\"Total trainable parameters: {total_params:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['wqkv', 'wo','w1','w2','w3'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze LoRA and v_head parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if \"lora_\" in name or \"v_head\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Optionally, you can print out which parameters are trainable\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Trainable parameter: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='output-h100-1',\n",
    "    overwrite_output_dir=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=25,  # Save every 25 steps\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,  # Log every 10 steps\n",
    "    warmup_steps=10,  # Warmup steps\n",
    "    optim=\"adamw_8bit\",\n",
    "    learning_rate=5e-6,  # Learning rate\n",
    "    per_device_train_batch_size=16,  # Batch size of 16 sequences\n",
    "    per_device_eval_batch_size=32, \n",
    "    gradient_accumulation_steps=1,  # Effectively no gradient accumulation\n",
    "    max_grad_norm=0.5,  # Gradient clipping\n",
    "    num_train_epochs=1,  # Number of epochs\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Mixed precision training\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## Starting to train ##\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'output/H100-full_model_pytorch-1'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save the full model, including the v_head layer\n",
    "torch.save(model.state_dict(), os.path.join(save_path, 'pytorch_model.bin'))\n",
    "\n",
    "# Save the configuration as well\n",
    "model.config.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()\n",
    "model.save_pretrained('H100-merged-1')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
